legend("topleft",lty=1,col=c(4,2,3),
legend=c("Mean Method","Naive Method","Drift Method"))
par(mfrow=c(1,1))
hsales_avg <- meanf(hsales_train,h=24)$mean
hsales_naive <- naive(hsales_train,h=24)$mean
hsales_drift <- rwf(hsales_train,drift=TRUE,h=24)$mean
plot(hsales_train,main="House Sales",xlab="Month",ylab="Price")
lines(hsales_naive,col=2)
lines(hsales_avg,col=4)
lines(hsales_drift,col=3)
lines(hsales_test,col=8)
legend("topleft",lty=1,col=c(4,2,3),
legend=c("Mean Method","Naive Method","Drift Method"))
plot(hsales_train,main="House sales", ylab="House Sales",xlab="Month", xlim=c(240,275), ylim=c(35,75))
lines(hsales_naive,col=2)
lines(hsales_avg,col=4)
lines(hsales_drift,col=3)
lines(hsales_test,col=8)
legend("topleft",lty=1,col=c(4,2,3), legend=c("Mean Method","Naive Method","Drift Method"))
econsumption
plot(Mwh ~ temp, data = econsumption, main = "Econsumption")
fit = lm(formula = Mwh  ~ temp, data = econsumption)
abline(fit, col=5)
summary(fit)
par(mfrow=c(2,2))
plot(fit)
summary(fit)
par(mfrow=c(2,2))
plot(fit)
coeffs = coefficients(fit)
pred_temp = c(10, 35)
p_temp = coeffs[1] + coeffs[2]*pred_temp
p_temp
coeffs = coefficients(fit)
pred_temp = c(10, 35)
p_temp = coeffs[1] + coeffs[2]*pred_temp
p_temp
par(mfrow=c(1,2))
fcast <- forecast(fit, newdata=data.frame(temp=10))
plot(fcast, xlab="temp", ylab="Mwh")
fcast2 <- forecast(fit, newdata=data.frame(temp=35))
plot(fcast2, xlab="temp", ylab="Mwh")
par(mfrow=c(2,1))
fcast <- forecast(fit, newdata=data.frame(temp=10))
plot(fcast, xlab="temp", ylab="Mwh")
fcast2 <- forecast(fit, newdata=data.frame(temp=35))
plot(fcast2, xlab="temp", ylab="Mwh")
temp10 = data.frame(temp=10)
temp35 = data.frame(temp=35)
predict(fit, temp10, interval="predict")
temp10 = data.frame(temp=10)
temp35 = data.frame(temp=35)
predict(fit, temp10, interval="predict")
predict(fit, temp35, interval="predict")
olympic1 <- matrix(c(1896, 54.2, 1900, 49.4, 1904, 49.2, 1908, 50, 1912 , 48.2, 1920 , 49.6, 1924 , 47.6 , 1928 , 47.8, 1932, 46.2, 1936, 46.5, 1948, 46.2, 1952, 45.9, 1956, 46.7, 1960, 44.9, 1964, 45.1, 1968, 43.8 , 1972, 44.66, 1976, 44.26, 1980, 44.6, 1984, 44.27, 1988 , 43.87 , 1992, 43.5, 1996 , 43.49 , 2000, 43.84 , 2004, 44, 2008, 43.75, 2012, 43.94, 2016 , 43.03) ,ncol=2,byrow=TRUE)
colnames(olympic1) <- c("Year","time")
olympic_ts <- ts(olympic1,start=1,end=28)
olympic1 <- matrix(c(1896, 54.2, 1900, 49.4, 1904, 49.2, 1908, 50, 1912 , 48.2, 1920 , 49.6, 1924 , 47.6 , 1928 , 47.8, 1932, 46.2, 1936, 46.5, 1948, 46.2, 1952, 45.9, 1956, 46.7, 1960, 44.9, 1964, 45.1, 1968, 43.8 , 1972, 44.66, 1976, 44.26, 1980, 44.6, 1984, 44.27, 1988 , 43.87 , 1992, 43.5, 1996 , 43.49 , 2000, 43.84 , 2004, 44, 2008, 43.75, 2012, 43.94, 2016 , 43.03) ,ncol=2,byrow=TRUE)
colnames(olympic1) <- c("Year","time")
olympic_ts <- ts(olympic1,start=1,end=28)
plot(time ~ Year, data = olympic_ts, main = "Olympic Gold Medal Times")
plot(time ~ Year, data = olympic_ts, main = "Olympic Gold Medal Times")
fit1 = lm(formula = time  ~ Year, data = olympic_ts)
plot(time ~ Year, data = olympic_ts, main = "Olympic Gold Medal Times")
abline(fit1, col=5)
summary(fit1)
par(mfrow=c(2,2))
plot(fit1)
coeffs1 = coefficients(fit1)
pred_time = c(2000, 2004, 2008, 2012)
p_time = coeffs1[1] + coeffs1[2]*pred_time
p_time
par(mfrow=c(2,2))
fcast3 <- forecast(fit1, newdata=data.frame(Year=2000))
plot(fcast3, xlab="Year", ylab="time")
fcast4 <- forecast(fit1, newdata=data.frame(Year=2004))
plot(fcast4, xlab="Year", ylab="time")
fcast5 <- forecast(fit1, newdata=data.frame(Year=2008))
plot(fcast5, xlab="Year", ylab="time")
fcast6 <- forecast(fit1, newdata=data.frame(Year=2012))
plot(fcast6, xlab="Year", ylab="time")
par(mfrow=c(2,2))
fcast3 <- forecast(fit1, newdata=data.frame(Year=2000))
plot(fcast3, xlab="Year", ylab="time")
fcast4 <- forecast(fit1, newdata=data.frame(Year=2004))
plot(fcast4, xlab="Year", ylab="time")
fcast5 <- forecast(fit1, newdata=data.frame(Year=2008))
plot(fcast5, xlab="Year", ylab="time")
fcast6 <- forecast(fit1, newdata=data.frame(Year=2012))
plot(fcast6, xlab="Year", ylab="time")
time2000 = data.frame(Year=2000)
time2004 = data.frame(Year=2004)
time2008 = data.frame(Year=2008)
time2012 = data.frame(Year=2012)
predict(fit1, time2000, interval="predict")
predict(fit1, time2004, interval="predict")
predict(fit1, time2008, interval="predict")
predict(fit1, time2004, interval="predict")
predict(fit1, time2012, interval="predict")
olympic_ts[24:27,2]
coeffs1 = coefficients(fit1)
pred_time = c(2000, 2004, 2008, 2012)
p_time = coeffs1[1] + coeffs1[2]*pred_time
p_time
par(mfrow=c(1,1))
plot(fit1)
plot(time ~ Year, data = olympic_ts, main = "Olympic Gold Medal Times")
abline(fit1, col=5)
econsumption
library(fma)
econsumption
#seasonal component present in this time series.Mean method is worse than the Drift and the Naive method. The variability in the data is making the forcasts unable to be reliable results. Also the Drift and Naive method are producing nearly indistinguishable results.
walmart_store <- read.csv("C:/Users/amber/Desktop/features.csv/walmart_store.csv")
View(walmart_store)
features <- read.csv("C:/Users/amber/Desktop/features.csv/features.csv", header=FALSE)
View(features)
View(walmart_store)
View(features)
rm(features)
features <- read.csv("C:/Users/amber/Desktop/features.csv/features.csv")
View(features)
walmart_store <- subset( walmart_store, select = -X )
walmart_store_data <- merge(walmart_store, features, by = "Store", all.y = TRUE)
gc()
memory.size(max=T)
walmart_store_data <- merge(walmart_store, features, by = "Store", all.y = TRUE)
View(walmart_store)
walmart_store1 <- combi[1:200000,]
gc()
memory.size(max=T)
walmart_store1 <- walmart_store[1:200000,]
gc()
walmart_store_data <- merge(walmart_store1, features, by = "Store", all.y = TRUE)
View(walmart_store_data)
write.csv(walmart_store_data, "C:\Users\amber\Desktop\features.csv\walmart_data")
write.csv(walmart_store_data, "C:\\Users\\amber\\Desktop\\features.csv\\walmart_data.csv")
#Including the required library
library(fma)
econsumption
plot(Mwh ~ temp, data = econsumption, main = "Econsumption")
fit = lm(formula = Mwh  ~ temp, data = econsumption)
abline(fit, col=5)
summary(fit)
plot(Mwh ~ temp, data = econsumption, main = "Fig 1: Econsumption")
fit = lm(formula = Mwh  ~ temp, data = econsumption)
abline(fit, col=5)
summary(fit)
par(mfrow=c(2,2))
plot(fit)
par(mfrow=c(1,1))
plot(time ~ Year, data = olympic_ts, main = "Olympic Gold Medal Times")
summary(fit)
par(mfrow=c(2,2))
plot(fit)
coeffs = coefficients(fit)
pred_temp = c(10, 35)
p_temp = coeffs[1] + coeffs[2]*pred_temp
p_temp
coeffs = coefficients(fit)
pred_temp = c(10, 35)
p_temp = coeffs[1] + coeffs[2]*pred_temp
p_temp
par(mfrow=c(2,1))
fcast <- forecast(fit, newdata=data.frame(temp=10))
plot(fcast, xlab="temp", ylab="Mwh")
fcast2 <- forecast(fit, newdata=data.frame(temp=35))
plot(fcast2, xlab="temp", ylab="Mwh")
temp10 = data.frame(temp=10)
temp35 = data.frame(temp=35)
predict(fit, temp10, interval="predict")
predict(fit, temp35, interval="predict")
predict(fit, temp35, interval="predict")
par(mfrow=c(1,1))
olympic1 <- matrix(c(1896, 54.2, 1900, 49.4, 1904, 49.2, 1908, 50, 1912 , 48.2, 1920 , 49.6, 1924 , 47.6 , 1928 , 47.8, 1932, 46.2, 1936, 46.5, 1948, 46.2, 1952, 45.9, 1956, 46.7, 1960, 44.9, 1964, 45.1, 1968, 43.8 , 1972, 44.66, 1976, 44.26, 1980, 44.6, 1984, 44.27, 1988 , 43.87 , 1992, 43.5, 1996 , 43.49 , 2000, 43.84 , 2004, 44, 2008, 43.75, 2012, 43.94, 2016 , 43.03) ,ncol=2,byrow=TRUE)
colnames(olympic1) <- c("Year","time")
olympic_ts <- ts(olympic1,start=1,end=28)
par(mfrow=c(1,1))
plot(time ~ Year, data = olympic_ts, main = "Olympic Gold Medal Times")
dengue_features_train <- read.csv("F:/OneDrive - Texas Tech University/MastersDocuments/DS-Predictive Analytics/DengAI/DataSet/dengue_features_train.csv")
View(dengue_features_train)
output: html_document
library(forecast)
library(fpp)
library(TTR)
library(forecast)
library(fpp)
library(TTR)
install.packages("TTR")
library(forecast)ttr
library(fpp)
library(TTR)
library(forecast)
library(fpp)
library(TTR)
library(forecast)
library(fpp)
library(knitr)
install.packages("knitr")
library(fpp)
library(knitr)
#Including the required library dole - Unemployment benefits in Australia
library(knitr)
library(fpp)
summary(fit15)
plot(books[,1], main="paperback sales", xlab="days", ylab="money", xlim=c(0,40))
books
dev.off()
fit15 <- holt(books[,1], initial = "simple", h=4)
summary(fit15)
plot(books[,1], main="paperback sales", xlab="days", ylab="money", xlim=c(0,40))
lines(fitted(fit15), col="red", type="o")
lines(fit15$mean, col="green", type="o")
fit15<- holt(books[,1], initial = "optimal", h=4)
summary(fit15)
plot(books[,1], main="paperback sales", xlab="days", ylab="money", xlim=c(0,40))
lines(fitted(fit15), col="blue", type="o")
lines(fit15$mean, col="blue", type="o")
books
dev.off()
fit15 <- holt(books[,1], initial = "simple", h=4)
summary(fit15)
plot(books[,1], main="paperback sales", xlab="days", ylab="money", xlim=c(0,40))
plot(books[,1], main="paperback sales", xlab="days", ylab="money", xlim=c(0,40))
lines(fitted(fit15), col="blue", type="o")
lines(fit15$mean, col="blue", type="o")
summary(fit15)
plot(books[,1], main="paperback sales", xlab="days", ylab="money", xlim=c(0,40))
#Including the required library dole - Unemployment benefits in Australia
library(knitr)
library(fpp)
books <- books
plot(books)
pb <- books[,1]
fit1 <- ses(pb, initial='simple', varalpha=0.2, h=3)
sum((pb - fitted(fit1))) #51.94
fit2 <- ses(pb, initial='simple', varalpha=0.4, h=3)
sum((pb - fitted(fit2))) #50.28
fit3 <- ses(pb, initial='simple', varalpha=0.6, h=3)
sum((pb - fitted(fit3))) #47.45
fit4 <- ses(pb, initial='simple', varalpha=0.8, h=3)
sum((pb - fitted(fit4))) #46.67
fit5 <- ses(pb, initial='simple', varalpha=0.9, h=3)
sum((pb - fitted(fit5))) #47.10 - previous sse was smaller so choose smaller varalpha.
fit6 <- ses(pb, initial='simple', varalpha=0.85, h=3)
sum((pb - fitted(fit6))) #46.82 - previous sse was smaller so choose smaller varalpha.
fit7 <- ses(pb, initial='simple', varalpha=0.83, h=3)
sum((pb - fitted(fit7))) #46.74 - previous sse was smaller so choose smaller varalpha.
fit8 <- ses(pb, initial='simple', varalpha=0.82, h=3)
sum((pb - fitted(fit8))) #46.71 - previous sse was smaller so choose smaller varalpha.
fit9 <- ses(pb, initial='simple', varalpha=0.81, h=3)
sum((pb - fitted(fit9))) #46.69 - previous sse was smaller so choose smaller varalpha.
fit10 <- ses(pb, initial='simple', varalpha=0.75, h=3)
sum((pb - fitted(fit10))) #46.66 - new minimum found.
fit11 <- ses(pb, initial='simple', varalpha=0.3, h=3)
sum((pb - fitted(fit11))) #52.24
fit12 <- ses(pb, initial='simple', varalpha=0.5, h=3)
sum((pb - fitted(fit12))) #48.62
fit13 <- ses(pb, initial='simple', varalpha=1, h=3)
sum((pb - fitted(fit13))) #48
varalpha <- c(0.2, 0.4, 0.6, 0.8, 0.9, 0.85, 0.83, 0.82, 0.81, 0.75, 0.3, 0.5, 1)
sse <- c(51.94, 50.28, 47.45, 46.67, 47.10, 46.82, 46.74, 46.71, 46.68, 46.66, 52.24, 48.62, 48)
plot(varalpha, sse)
fit1 <- ses(pb, initial='simple', h=4)
fit2 <- ses(pb, initial='simple', varalpha=0.75, h=4)
par(mfrow=c(2,1))
plot(fit1, main="Automatic varalpha")
plot(fit2, main="Custom varalpha: 0.75")
fit3 <- ses(pb, initial='optimal', h=4)
sum((pb - fitted(fit3)))
hb <- books[,2]
fit1 <- ses(hb, initial='simple', varalpha=0.8, h=3)
sum((hb - fitted(fit1))) #142.59
fit2 <- ses(hb, initial='simple', varalpha=0.2, h=3)
sum((hb - fitted(fit2))) #465.09
fit3 <- ses(hb, initial='simple', varalpha=0.9, h=3)
sum((hb - fitted(fit3))) #129.61
fit4 <- ses(hb, initial='simple', varalpha=0.5, h=3)
sum((hb - fitted(fit4))) #213.47
fit5 <- ses(hb, initial='simple', varalpha=0.95, h=3)
sum((hb - fitted(fit5))) #124.42
fit6 <- ses(hb, initial='simple', varalpha=0.97, h=3)
sum((hb - fitted(fit6))) #122.56
fit7 <- ses(hb, initial='simple', varalpha=0.99, h=3)
sum((hb - fitted(fit7))) #120.82
fit8 <- ses(hb, initial='simple', varalpha=1, h=3)
sum((hb - fitted(fit8))) #120
varalpha <- c(1, 0.99, 0.97, 0.95, 0.5, 0.9, 0.2, 0.8)
sse <- c(120, 120.82, 122.56, 124.42, 213.47, 129.61, 465.09, 142.59)
plot(varalpha, sse)
par(mfrow=c(2,1))
plot(fit1, main="Automatic varalpha")
plot(fit2, main="Custom varalpha: 1")
fit3 <- ses(hb, initial='optimal', h=4)
sum((hb - fitted(fit3)))
books
dev.off()
fit15 <- holt(books[,1], initial = "simple", h=4)
summary(fit15)
plot(books[,1], main="paperback sales", xlab="days", ylab="money", xlim=c(0,40))
lines(fitted(fit15), col="red", type="o")
lines(fit15$mean, col="green", type="o")
fit15<- holt(books[,1], initial = "optimal", h=4)
summary(fit15)
fit15<- holt(books[,1], initial = "optimal", h=4)
summary(fit15)
plot(books[,1], main="paperback sales", xlab="days", ylab="money", xlim=c(0,40))
lines(fitted(fit15), col="blue", type="o")
lines(fit15$mean, col="blue", type="o")
plot(fit15, xlab="days", ylab= "Money")
plot(fit15, xlab="days", ylab="money")
pb <- ets(paperback)
plot(forecast(pb, h=4))
pb <- ets(hb)
plot(forecast(pb, h=4))
decomposed <- stl(cars, s.window="periodic", robust=TRUE)
seasonal <- decomposed$time.series[,1]
cars_sa <- cars - seasonal
fit1 <- holt(cars_sa, h=8, damped = TRUE)
lastyear <- rep(decomposed$time.series[110:113,"seasonal"],2)
reseasonalized_fc <- fit$mean + lastyear
lines(fitted(fit15), col="red", type="o")
lines(fit15$mean, col="green", type="o")
fit15<- holt(books[,1], initial = "optimal", h=4)
summary(fit15)
plot(books[,1], main="paperback sales", xlab="days", ylab="money", xlim=c(0,40))
lines(fitted(fit15), col="blue", type="o")
lines(fit15$mean, col="blue", type="o")
plot(fit15, xlab="days", ylab= "Money")
plot(fit15, xlab="days", ylab="money")
pb <- ets(paperback)
plot(forecast(pb, h=4))
pb <- ets(hb)
plot(forecast(pb, h=4))
pb <- ets(paperback)
etspb <- ets(paperback)
plot(forecast(etspb, h=4))
etshb <- ets(hb)
plot(forecast(etshb, h=4))
etspb <- ets(paperback)
etspb <- ets(paperback)
plot(forecast(etspb, h=4))
etshb <- ets(hb)
plot(forecast(etshb, h=4))
cars <- ukcars
plot(ukcars)
decomposed <- stl(cars, s.window="periodic", robust=TRUE)
seasonal <- decomposed$time.series[,1]
cars_sa <- cars - seasonal
fit1 <- holt(cars_sa, h=8, damped = TRUE)
lastyear <- rep(decomposed$time.series[110:113,"seasonal"],2)
reseasonalized_fc <- fit$mean + lastyear
fit1 <- holt(cars_sa, h=8, damped = TRUE)
lastyear <- rep(decomposed$time.series[110:113,"seasonal"],2)
reseasonalized_fc <- fit1$mean + lastyear
dengue_features_train <- read.csv("F:/OneDrive - Texas Tech University/MastersDocuments/DS-Predictive Analytics/DengAI/DataSet/dengue_features_train.csv")
View(dengue_features_train)
dengue_features_test <- read.csv("F:/OneDrive - Texas Tech University/MastersDocuments/DS-Predictive Analytics/DengAI/DataSet/dengue_features_test.csv")
View(dengue_features_test)
dengue_labels_train <- read.csv("F:/OneDrive - Texas Tech University/MastersDocuments/DS-Predictive Analytics/DengAI/DataSet/dengue_labels_train.csv")
View(dengue_labels_train)
#is.na(dengue_features_train$ndvi_ne)
colSums(is.na(dengue_features_train))
for(i in 1:ncol(dengue_features_train)){
dengue_features_train[is.na(dengue_features_train[,i]), i] <- mean(dengue_features_train[,i], na.rm = TRUE)
}
colSums(is.na(dengue_features_test))
for(i in 1:ncol(dengue_features_test)){
dengue_features_test[is.na(dengue_features_test[,i]), i] <- mean(dengue_features_test[,i], na.rm = TRUE)
}
total <- merge(dengue_features_train,dengue_labels_train,by=c("city","year","weekofyear"))
library(h2o)
localH2O = h2o.init()
set.seed(2)
train_h2o <- as.h2o(total)
test_h2o  <- as.h2o(dengue_features_test)
timer <- proc.time()
system.time(
dlearning.model <- h2o.deeplearning(                                     # data in H2O format
x=2:24,
y=25,
activation = "RectifierWithDropout",
#input_dropout_ratio = 0.2, # % of inputs dropout
# hidden_dropout_ratios = c(0.5,0.5,0.5), # % for nodes dropout
training_frame=train_h2o,
nfolds=10,
#validation_frame=valid_index.hex,
epochs=50,
hidden=c(18,18)# more hidden layers -> more complex interactions
# helps stability for Rectifier
)
)
pred_value <- predict(dlearning.model, test_h2o)
pred_value_df= as.data.frame(pred_value)
write.csv(pred_value_df,"D://seema.csv")
class(dengue_labels_train)
train <- read.csv("F:/OneDrive - Texas Tech University/MastersDocuments/DS-Predictive Analytics/PCA/Train.csv")
test <- read.csv("F:/OneDrive - Texas Tech University/MastersDocuments/DS-Predictive Analytics/PCA/Test.csv")
#https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/
path <- "F:\\OneDrive - Texas Tech University\\MastersDocuments\\DS-Predictive Analytics\\PCA"
setwd(path)
train <- read.csv("F:/OneDrive - Texas Tech University/MastersDocuments/DS-Predictive Analytics/PCA/Train.csv")
test <- read.csv("F:/OneDrive - Texas Tech University/MastersDocuments/DS-Predictive Analytics/PCA/Test.csv")
#add a column
test$Item_Outlet_Sales <- 1
#combine the data set
combi <- rbind(train, test)
#impute missing values with median
combi$Item_Weight[is.na(combi$Item_Weight)] <- median(combi$Item_Weight, na.rm = TRUE)
#impute 0 value of Item_Visibility with median
combi$Item_Visibility <- ifelse(combi$Item_Visibility == 0, median(combi$Item_Visibility),combi$Item_Visibility)
for (i in 1:ncol(combi)){
print(i)
print(sum(is.na(combi[i])))
}
#find mode and impute
table(combi$Outlet_Size, combi$Outlet_Type)
levels(combi$Outlet_Size)[1] <- "Other"
str(combi)
#left with removing the dependent (response) variable and other identifier variables( if any).
my_data <- subset(combi, select = -c(Item_Outlet_Sales, Item_Identifier,Outlet_Identifier))
#Letâs check the available variables ( a.k.a predictors) in the data set.
#check available variables
colnames(my_data)
#Since PCA works on numeric variables, letâs see if we have any variable other than numeric.
#check variable class
str(my_data)
#Sadly, 6 out of 9 variables are categorical in nature. We have some additional work to do now. Weâll convert these categorical variables into numeric using one hot encoding.
#load library
library(dummies)
#create a dummy data frame
new_my_data <- dummy.data.frame(my_data, names = c("Item_Fat_Content","Item_Type",
"Outlet_Establishment_Year","Outlet_Size",
"Outlet_Location_Type","Outlet_Type"))
#divide the new data
pca.train <- new_my_data[1:nrow(train),]
pca.test <- new_my_data[-(1:nrow(train)),]
#The base R function prcomp() is used to perform PCA. By default, it centers the variable to have mean equals to zero.
#With parameter scale. = T, we normalize the variables to have standard deviation equals to 1.
#principal component analysis
prin_comp <- prcomp(pca.train, scale. = T)
names(prin_comp)
#1. center and scale refers to respective mean and standard deviation of the variables that are used for normalization prior to implementing PCA
#outputs the mean of variables
prin_comp$center
#outputs the standard deviation of variables
prin_comp$scale
#2. The rotation measure provides the principal component loading. Each column of rotation matrix contains the
#principal component loading vector. This is the most important measure we should be interested in.
prin_comp$rotation
#This returns 44 principal components loadings. Is that correct ? Absolutely.
#In a data set, the maximum number of principal component loadings is a minimum of (n-1, p).
#Letâs look at first 4 principal components and first 5 rows.
prin_comp$rotation[1:5,1:4]
#the matrix x has the principal component score vectors in a 8523 Ã 44 dimension.
dim(prin_comp$x)
biplot(prin_comp, scale = 0)
#4. The prcomp() function also provides the facility to compute standard deviation of each principal component.
#sdev refers to the standard deviation of principal components.
#compute standard deviation of each principal component
std_dev <- prin_comp$sdev
#compute variance
pr_var <- std_dev^2
#check variance of first 10 components
pr_var[1:10]
#So, higher is the explained variance, higher will be the information contained in those components.
#proportion of variance explained
prop_varex <- pr_var/sum(pr_var)
prop_varex[1:20]
#cumulative scree plot
plot(cumsum(prop_varex), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
type = "b")
#This plot shows that 30 components results in variance close to ~ 98%. Therefore, in this case,
#weâll select number of components as 30 [PC1 to PC30] and proceed to the modeling stage. This completes the steps to
#implement PCA on train data. For modeling, weâll use these 30 components as predictor variables and follow the normal procedures.
#add a training set with principal components
train.data <- data.frame(Item_Outlet_Sales = train$Item_Outlet_Sales, prin_comp$x)
#we are interested in first 30 PCAs
train.data <- train.data[,1:31]
#run a decision tree
install.packages("rpart")
library(rpart)
rpart.model <- rpart(Item_Outlet_Sales ~ .,data = train.data, method = "anova")
rpart.model
#transform test into PCA
test.data <- predict(prin_comp, newdata = pca.test)
test.data <- as.data.frame(test.data)
#select the first 30 components
test.data <- test.data[,1:30]
#make prediction on test data
rpart.prediction <- predict(rpart.model, test.data)
sample <- read.csv("F:/OneDrive - Texas Tech University/MastersDocuments/DS-Predictive Analytics/PCA/SampleSubmission_TmnO39y.csv")
final.sub <- data.frame(Item_Identifier = sample$Item_Identifier, Outlet_Identifier = sample$Outlet_Identifier, Item_Outlet_Sales = rpart.prediction)
write.csv(final.sub, "F:/OneDrive - Texas Tech University/MastersDocuments/DS-Predictive Analytics/PCA/pca.csv",row.names = F)
View(dengue_features_test)
View(combi)
str(my_data)
pca.train
ncol(pca.test)
ncol(pca.train)
View(pca.test)
View(pca.train)
str(pca.test)
View(test)
View(train)
View(test)
View(pca.train)
prin_comp$x
install.packages("tidyverse")
install.packages("corrplot")
install.packages("zoo")
install.packages("MASS")
